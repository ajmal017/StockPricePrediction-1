# -*- coding: utf-8 -*-
"""feature_selection_trees.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NzlLzt3Kb6yigXZ4FQN6BXvsCgPzpVHQ
"""

import xgboost as xgb
from sklearn.impute import SimpleImputer
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error


def get_X_y(data):
    data = data.copy()
    # X = data.iloc[:, 3:]
    # X = X.join(data.iloc[:, 1])
    X = np.c_[data[:, 3:], data[:, 1]]
    y = data[:, 2]
    print(X.shape)
    return X, y


def get_sliding_windows(X, y, lookback, delay, min_index, max_index):
    f = False
    for i in range(min_index, max_index):
        if not f:
            samples = X[i:i + lookback].flatten()
            samples = samples.reshape((1, samples.shape[0]))
            targets = y[i + lookback:i + lookback + delay]
            f = True
        else:
            temp = X[i:i + lookback].flatten()
            samples = np.r_[samples, temp.reshape((1, temp.shape[0]))]
            targets = np.r_[targets, y[i + lookback:i + lookback + delay]]
    return samples, targets


def get_random_forrest_model(samples, targets):
    regressor = RandomForestRegressor(criterion='mae', n_estimators=50, verbose=True, n_jobs=-1)
    model = regressor.fit(samples, targets)
    return regressor


def get_xgb_model(samples, targets, samples_eval, targets_eval):
    regressor = xgb.XGBRegressor()
    model = regressor.fit(samples, targets, early_stopping_rounds=3, eval_metric="mae",
                          eval_set=[(samples, targets), (samples_eval, targets_eval)], verbose=True)
    return regressor


def split_data(data):
    data_new = data.drop(['fft200', 'fft100', 'fft50'], axis=1)
    values = data_new.to_numpy()
    X, y = get_X_y(values)
    imp_mean = SimpleImputer(missing_values=np.nan, strategy='median')
    imp_mean.fit(X)
    X = imp_mean.transform(X)
    lookback = 10

    cut = 1
    if cut:
        # cutoff first 700 data points
        X = X[700:]
        y = y[700:]
    train_size = int(0.8 * X.shape[0])
    print(X.shape[0])
    samples, targets = get_sliding_windows(X, y, lookback, 1, 0, train_size)
    samples_eval, targets_eval = get_sliding_windows(X, y, lookback, 1, train_size, X.shape[0] - lookback)
    return (samples, targets), (samples_eval, targets_eval)


def select_features(data):
    (samples, targets), (samples_eval, targets_eval) = split_data(data)

    # model = get_random_forrest_model(samples, targets)
    model = get_xgb_model(samples, targets, samples_eval, targets_eval)

    y_pred = model.predict(samples_eval)
    targets_list = targets_eval.tolist()
    print('Targets    ', targets_list[-1:])
    predictions = [round(y, 2) for y in y_pred]
    print('predictions', predictions[-1:])
    fig = plt.figure(figsize=(20, 8))
    plt.plot(targets_list, label='targets')
    plt.plot(predictions, label='predictions')
    plt.title('Targets vs predicitons')
    plt.legend()
    mse = mean_squared_error(targets_list, predictions)
    mae = mean_absolute_error(targets_list, predictions)
    print("MSE: ", mse)
    print("MAE: ", mae)


def plot_feature_importance_2(data, model, lookback):
    feature_labels = []
    features = data.iloc[:, 3:]
    features = features.join(data.iloc[:, 1])
    for i in range(lookback, 0, -1):
        feature_labels += [feature + '_' + str(i) for feature in features.columns]
    print('n_features: ', len(feature_labels))
    import matplotlib
    matplotlib.rc('xtick', labelsize=10)
    matplotlib.rc('ytick', labelsize=10)

    fig = plt.figure(figsize=(10, 100))
    plt.xticks(rotation='vertical')
    # fig, ax = plt.subplots()
    feature_labels = np.asarray(feature_labels)
    importance = model.feature_importances_
    indices = np.argsort(importance)
    range_ = range(len(importance))
    plt.barh(range_, importance[indices])
    plt.yticks(range(len(importance)), feature_labels[indices])
    # plt.barh([i for i in range(len(model.feature_importances_))], model.feature_importances_.tolist(), tick_label=feature_labels)
    plt.title('Feature importance')
    plt.show()


def plot_features_importance(data, model, lookback):
    feature_labels = []
    features = data.iloc[:, 3:]
    features = features.join(data.iloc[:, 1])
    for i in range(lookback, 0, -1):
        feature_labels += [feature + '_' + str(i) for feature in features.columns]
    print('n_features: ', len(feature_labels))
    import matplotlib
    matplotlib.rc('xtick', labelsize=10)
    matplotlib.rc('ytick', labelsize=10)
    feature_labels = np.asarray(feature_labels)
    fig = plt.figure(figsize=(10, 100))
    plt.xticks(rotation='vertical')
    # fig, ax = plt.subplots()
    importance = np.max([tree.feature_importances_ for tree in model.estimators_], axis=0)
    indices = np.argsort(importance)
    range_ = range(len(importance))
    plt.barh(range_, importance[indices])
    plt.yticks(range(len(importance)), feature_labels[indices])
    # plt.barh([i for i in range(len(model.feature_importances_))], model.feature_importances_.tolist(), tick_label=feature_labels)
    plt.title('Figure 6: Feature importance of the technical indicators.')
    plt.xlim([0.0, 0.1])
    plt.show()


def plot_validation_vs_training(model):
    eval_result = model.evals_result()
    training_rounds = range(len(eval_result['validation_0']['rmse']))

    plt.scatter(x=training_rounds, y=eval_result['validation_0']['rmse'], label='Training Error')
    plt.scatter(x=training_rounds, y=eval_result['validation_1']['rmse'], label='Validation Error')
    plt.xlabel('Iterations')
    plt.ylabel('RMSE')
    plt.title('Training Vs Validation Error')
    plt.legend()
    plt.show()

# def generator(data, lookback, delay, min_index, max_index, shuffle=False, batch_size=128, step=6):
#     if max_index is None:
#         max_index = len(data) - delay - 1
#     i = min_index + lookback
#     while 1:
#         if shuffle:
#             rows = np.random.randint(min_index + lookback, max_index, size=batch_size)
#         else:
#             if i + batch_size >= max_index:
#                 i = min_index + lookback
#             rows = np.arange(i, min(i + batch_size, max_index))
#             i += len(rows)
#         samples = np.zeros((len(rows),lookback // step,data.shape[-1]))
#         targets = np.zeros((len(rows),))
#         for j, row in enumerate(rows):
#             indices = range(rows[j] - lookback, rows[j], step)
#             samples[j] = data[indices]
#             targets[j] = data[rows[j] + delay][1]
#         yield samples, targets
